{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQ/L9T2yzuIW++suozwv0l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pavansai26/Attention-mechanisms/blob/main/Multihead_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Multihead Attention\n",
        "\n"
      ],
      "metadata": {
        "id": "aIe-6yE0l1tj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#A mechanism that allows a model to attend to different parts of an input sequence multiple times, each time with a different focus."
      ],
      "metadata": {
        "id": "Jp-J-i-Bl6pT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Steps:\n"
      ],
      "metadata": {
        "id": "bWJ8qnbNl-9I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Split and Project: The input is split into multiple heads, and each head is projected into separate query, key, and value matrices.\n",
        "\n",
        "#Scaled Dot-Product Attention: For each head, attention scores are calculated using scaled dot-product attention between queries and keys.\n",
        "\n",
        "#Softmax and Weighted Sum: The scores are normalized using softmax, and a weighted sum of values is computed based on the attention scores.\n",
        "\n",
        "#Concatenate and Project: The outputs from all heads are concatenated and projected into the final output."
      ],
      "metadata": {
        "id": "43Qn4vNcmEO4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Advantages:\n",
        "\n"
      ],
      "metadata": {
        "id": "W53-n42jmbH5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Captures Diverse Relationships: Allows the model to learn multiple, diverse relationships between elements in the input sequence.\n",
        "\n",
        "#Improves Representation Learning: Leads to richer, more informative representations of the input.\n",
        "\n",
        "#Enhances Parallelism: The heads can be calculated in parallel, improving computational efficiency.\n",
        "\n",
        "#Stabilizes Training: Helps prevent overfitting and improves model generalization."
      ],
      "metadata": {
        "id": "AQHe10wKmngH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Disadvantages:"
      ],
      "metadata": {
        "id": "HeRO3HOzm8lJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Increased Complexity: Adds more parameters and complexity to the model.\n",
        "\n",
        "#Potential Overfitting: If not regularized appropriately, it can lead to overfitting.\n",
        "\n",
        "#Interpretability: Can be harder to interpret the model's decision-making process."
      ],
      "metadata": {
        "id": "Asjd7NBWnA2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "\"\"\"\n",
        "MultiheadAttention class for implementing multihead attention in TensorFlow.\n",
        "\"\"\"\n",
        "class MultiheadAttention(tf.keras.layers.Layer):\n",
        "   \"\"\"\n",
        "   Initialize a MultiheadAttention layer.\n",
        "\n",
        "   Args:\n",
        "       d_model: The dimensionality of the input and output representations.\n",
        "       num_heads: The number of attention heads.\n",
        "   \"\"\"\n",
        "   def __init__(self, d_model, num_heads):\n",
        "       super().__init__()\n",
        "       self.d_model = d_model\n",
        "       self.num_heads = num_heads\n",
        "       self.head_dim = d_model // num_heads  # Dimensionality of each attention head\n",
        "\n",
        "       # Linear layers for projecting queries, keys, and values\n",
        "       self.wq = tf.keras.layers.Dense(d_model)  # Query projection\n",
        "       self.wk = tf.keras.layers.Dense(d_model)  # Key projection\n",
        "       self.wv = tf.keras.layers.Dense(d_model)  # Value projection\n",
        "       self.wo = tf.keras.layers.Dense(d_model)  # Output projection\n",
        "\n",
        "   \"\"\"\n",
        "   Perform multihead attention on the given inputs.\n",
        "\n",
        "   Args:\n",
        "       query: The query tensor.\n",
        "       key: The key tensor.\n",
        "       value: The value tensor.\n",
        "       mask: (optional) A mask to prevent attention to certain positions.\n",
        "\n",
        "   Returns:\n",
        "       The output of the multihead attention layer and the attention weights.\n",
        "   \"\"\"\n",
        "   def call(self, query, key, value, mask=None):\n",
        "       batch_size = tf.shape(query)[0]  # Get batch size\n",
        "\n",
        "       # Project queries, keys, and values using linear layers\n",
        "       q = self.wq(query)\n",
        "       k = self.wk(key)\n",
        "       v = self.wv(value)\n",
        "\n",
        "       # Reshape into multiple heads (batch_size, sequence_length, num_heads, head_dim)\n",
        "       q = tf.reshape(q, (batch_size, -1, self.num_heads, self.head_dim))\n",
        "       k = tf.reshape(k, (batch_size, -1, self.num_heads, self.head_dim))\n",
        "       v = tf.reshape(v, (batch_size, -1, self.num_heads, self.head_dim))\n",
        "\n",
        "       # Transpose to (batch_size, num_heads, sequence_length, head_dim)\n",
        "       q = tf.transpose(q, perm=[0, 2, 1, 3])\n",
        "       k = tf.transpose(k, perm=[0, 2, 1, 3])\n",
        "       v = tf.transpose(v, perm=[0, 2, 1, 3])\n",
        "\n",
        "       # Calculate scaled dot-product attention scores\n",
        "       scores = tf.matmul(q, k, transpose_b=True) / tf.math.sqrt(tf.cast(self.head_dim, tf.float32))\n",
        "\n",
        "       # Apply masking if provided\n",
        "       if mask is not None:\n",
        "           scores = tf.where(mask == 0, tf.fill(tf.shape(scores), -1e9), scores)\n",
        "\n",
        "       # Normalize scores with softmax\n",
        "       attention = tf.nn.softmax(scores, axis=-1)\n",
        "\n",
        "       # Compute weighted values\n",
        "       output = tf.matmul(attention, v)\n",
        "\n",
        "       # Transpose back to original shape (batch_size, sequence_length, num_heads, head_dim)\n",
        "       output = tf.transpose(output, perm=[0, 2, 1, 3])\n",
        "\n",
        "       # Reshape to (batch_size, sequence_length, d_model)\n",
        "       output = tf.reshape(output, (batch_size, -1, self.d_model))\n",
        "\n",
        "       # Final output projection\n",
        "       output = self.wo(output)\n",
        "\n",
        "       return output, attention\n"
      ],
      "metadata": {
        "id": "yguUkyYtmcC6"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eVs8PNgenvNT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}